{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation",
   "id": "81b46419869ea0a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook contains code to collect and process source data to construct ML dataset for the project.\n",
    "\n",
    "Content:\n",
    "- Configuration cell (defines configuration for the following processing)\n",
    "- Processing logic cell (contains functions with processing logic)\n",
    "- Data processing workflow cells (data processing steps):\n",
    "    1. Download a geographical shapefile with ZIP code service areas boundaries\n",
    "    2. Download MODIS Land Surface Temperature data\n",
    "    3. Download MODIS Normalized Difference Vegetation Index data\n",
    "    4. Download satellite meteo data\n",
    "    5. Download satellite Land Cover data\n",
    "    6. Download satellite Fractional Impervious Surface data\n",
    "    7. Download population density data\n",
    "    8. Daily local weather data by city\n",
    "    9. Process Land Surface Temperature data\n",
    "    10. Process Normalized Difference Vegetation Index data\n",
    "    11. Process satellite meteo data\n",
    "    12. Process population density data\n",
    "    13. Process local meteostat data\n",
    "    14. process Land Cover data\n",
    "    15. Process Normalized Difference Vegetation Index data\n",
    "    16. Assemble final dataset"
   ],
   "id": "1b8ddf234d08e667"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "327bcb2cd33f2209"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import warnings  \n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "NASA_EARTHDATA_TOKEN = \"\"\"<your_nasa_earthdata_token>\"\"\"\n",
    "\n",
    "PROJECT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "ZCTA_DATA_DIR = os.path.join(RAW_DATA_DIR, 'zcta')\n",
    "ZCTA_SHAPEFILE= os.path.join(ZCTA_DATA_DIR, 'tl_2024_us_zcta520.shp')\n",
    "LST_DATA_DIR = os.path.join(RAW_DATA_DIR, 'lst')\n",
    "VI_DATA_DIR = os.path.join(RAW_DATA_DIR, 'vi')\n",
    "DAYMET_DATA_DIR = os.path.join(RAW_DATA_DIR, 'daymet')\n",
    "LC_DATA_DIR = os.path.join(RAW_DATA_DIR, 'lc')\n",
    "FIS_DATA_DIR = os.path.join(RAW_DATA_DIR, 'fis')\n",
    "CENSUS_DATA_DIR = os.path.join(RAW_DATA_DIR, 'census')\n",
    "METEOSTAT_DATA_DIR = os.path.join(RAW_DATA_DIR, 'meteostat')\n",
    "\n",
    "PROCESSED_DATA_DIR=os.path.join(DATA_DIR, 'processed')\n",
    "PROCESSED_LST_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'lst')\n",
    "PROCESSED_VI_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'vi')\n",
    "PROCESSED_DAYMET_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'daymet')\n",
    "PROCESSED_CENSUS_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'census')\n",
    "PROCESSED_METEOSTAT_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'meteostat')\n",
    "PROCESSED_LC_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'lc')\n",
    "PROCESSED_FIS_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'fis')\n",
    "\n",
    "FINAL_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'final')\n",
    "\n",
    "CITIES = {\n",
    "    \"ALBUQUERQUE\": {\n",
    "        \"state\": \"NM\",\n",
    "        \"zipcodes\": [\n",
    "            '87101', '87102', '87103', '87104', '87105', '87106', '87107', '87108', '87109',\n",
    "            '87110', '87111', '87112', '87113', '87114', '87120', '87121', '87122', '87123',\n",
    "            '87131', '87151', '87153', '87154', '87158', '87174', '87176', '87181', '87184',\n",
    "            '87185', '87187', '87190', '87191', '87192', '87193', '87194', '87195', '87196',\n",
    "            '87197', '87198', '87199'\n",
    "        ],\n",
    "        \"modis\": [\"h09v05\"],\n",
    "        \"daymet\": [\"11197\"],\n",
    "        \"latitude\": 35.0844,\n",
    "        \"longitude\": -106.6504,\n",
    "    },\n",
    "    \"COLUMBUS\": {\n",
    "        \"state\": \"OH\",\n",
    "        \"zipcodes\": [\n",
    "            '43085', '43201', '43202', '43203', '43204', '43205', '43206', '43207', '43209',\n",
    "            '43210', '43211', '43212', '43213', '43214', '43215', '43217', '43219', '43220',\n",
    "            '43221', '43222', '43223', '43224', '43227', '43228', '43229', '43230', '43231',\n",
    "            '43232', '43235', '43240'\n",
    "        ],\n",
    "        \"modis\": [\"h11v05\"],\n",
    "        \"daymet\": [\"11569\", \"11749\"],\n",
    "        \"latitude\": 39.9612,\n",
    "        \"longitude\": -82.9988,\n",
    "    },\n",
    "    \"DENVER\": {\n",
    "        \"state\": \"CO\",\n",
    "        \"zipcodes\": [\n",
    "            '80202', '80203', '80204', '80205', '80206', '80207', '80209', '80210', '80211',\n",
    "            '80212', '80216', '80218', '80219', '80220', '80222', '80223', '80224', '80226',\n",
    "            '80227', '80230', '80231', '80232', '80235', '80236', '80237', '80238', '80239',\n",
    "            '80246', '80249', '80264', '80290', '80293', '80294', '80299'\n",
    "        ],\n",
    "        \"modis\": [\"h09v05\"],\n",
    "        \"daymet\":[\"11558\"],\n",
    "        \"latitude\": 39.7392,\n",
    "        \"longitude\": -104.9903,\n",
    "    },\n",
    "    \"LAS VEGAS\": {\n",
    "        \"state\": \"NV\",\n",
    "        \"zipcodes\": [\n",
    "            '89101', '89102', '89103', '89104', '89106', '89107', '89108', '89109', '89110',\n",
    "            '89117', '89118', '89119', '89120', '89121', '89122', '89123', '89124', '89128',\n",
    "            '89129', '89130', '89131', '89134', '89135', '89138', '89139', '89141', '89142',\n",
    "            '89143', '89144', '89145', '89146', '89147', '89148', '89149', '89156', '89161',\n",
    "            '89166', '89169', '89178', '89179', '89183'\n",
    "        ],\n",
    "        \"modis\": [\"h08v05\"],\n",
    "        \"daymet\": [\"11193\", \"11373\"],\n",
    "        \"latitude\": 36.1699,\n",
    "        \"longitude\": -115.1398,\n",
    "    },\n",
    "    \"SEATTLE\": {\n",
    "        \"state\": \"WA\",\n",
    "        \"zipcodes\": [\n",
    "            '98101', '98102', '98103', '98104', '98105', '98106', '98107', '98108', '98109',\n",
    "            '98112', '98115', '98116', '98117', '98118', '98119', '98121', '98122', '98125',\n",
    "            '98126', '98133', '98134', '98136', '98144', '98154', '98164', '98174', '98195',\n",
    "            '98199'\n",
    "        ],\n",
    "        \"modis\": [\"h09v04\"],\n",
    "        \"daymet\": [\"12269\"],\n",
    "        \"latitude\": 47.6062,\n",
    "        \"longitude\": -122.3321,\n",
    "    },\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Processing Logic",
   "id": "3ea79c89a5ee302c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:53:42.159503Z",
     "start_time": "2024-11-28T23:53:42.083594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_fs_name(fs_path):\n",
    "    return fs_path.split(\"/\")[-1]\n",
    "    \n",
    "def generate_date_chunks(years, chunk_size):\n",
    "    all_dates = []\n",
    "    for year in years:\n",
    "        start_date = datetime.date(year, 1, 1)\n",
    "        end_date = datetime.date(year, 12, 31)\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            chunk = []\n",
    "            for _ in range(chunk_size):\n",
    "                if current_date > end_date:\n",
    "                    break\n",
    "                chunk.append(current_date)\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "            all_dates.append(chunk)\n",
    "    return all_dates\n",
    "\n",
    "def download_zipcode_data():\n",
    "    zip_url = \"https://www2.census.gov/geo/tiger/TIGER2024/ZCTA520/tl_2024_us_zcta520.zip\"\n",
    "    output_dir = ZCTA_DATA_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Downloading {zip_url}...\")\n",
    "    response = requests.get(zip_url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    print(\"Extracting files...\")\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "        z.extractall(output_dir)\n",
    "\n",
    "def get_granule_links_by_coordinates(lat, lon, start_date, end_date, product_name):\n",
    "    api_url = \"https://cmr.earthdata.nasa.gov/search/granules.json\"\n",
    "    links = []\n",
    "    token = NASA_EARTHDATA_TOKEN\n",
    "    params = {\n",
    "        \"short_name\": product_name,\n",
    "        \"bounding_box\": f\"{lon-0.01},{lat-0.01},{lon+0.01},{lat+0.01}\",\n",
    "        \"temporal\": f\"{start_date}T00:00:00Z,{end_date}T23:59:59Z\",\n",
    "        \"page_size\": 20,\n",
    "        \"token\": token\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    granules = response.json().get(\"feed\", {}).get(\"entry\", [])\n",
    "    print(f\"Found {len(granules)} granules.\")\n",
    "    for granule in granules:\n",
    "        for link in granule[\"links\"]: \n",
    "            link_title = link.get(\"title\")\n",
    "            if link_title is not None and \"Download \" in link_title and \"hdf\" in link_title:\n",
    "                links.append(link[\"href\"])\n",
    "    return links\n",
    "\n",
    "def download_granule(granule_url, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    token = NASA_EARTHDATA_TOKEN\n",
    "    granule_filename = granule_url.split(\"/\")[-1]\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    with requests.get(granule_url, headers=headers, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        granule_path = os.path.join(output_dir, granule_filename)\n",
    "        with open(granule_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    \n",
    "def download_modis_data(product_name, output_dir):\n",
    "    years = [2022, 2023]\n",
    "    chunk_size = 20\n",
    "    chunks = generate_date_chunks(years, chunk_size)\n",
    "    for chunk in chunks:\n",
    "        chunk_str = [datetime.datetime.strftime(x, \"%Y-%m-%d\") for x in chunk]\n",
    "        start, end = chunk_str[0], chunk_str[-1]\n",
    "        for city, conf in CITIES.items():\n",
    "            granule_links = get_granule_links_by_coordinates(conf[\"latitude\"], conf[\"longitude\"], start, end, product_name=product_name)\n",
    "            for link in tqdm(granule_links, desc=f\"Downloading MOD11A1 files for {city} [{start}, {end}]\"):\n",
    "                download_granule(link, output_dir)\n",
    "                \n",
    "def download_lst_data():\n",
    "    download_modis_data(\"MOD11A1\", LST_DATA_DIR)\n",
    "\n",
    "def download_vi_data():\n",
    "    download_modis_data(\"MOD13A2\", VI_DATA_DIR)\n",
    "    \n",
    "def get_observation_date(hdf_file):\n",
    "    basename = os.path.basename(hdf_file)\n",
    "    parts = basename.split('.')\n",
    "    date_str = parts[1]  \n",
    "    year = int(date_str[1:5])       \n",
    "    doy = int(date_str[5:8])        \n",
    "    measurement_date = datetime.datetime(year, 1, 1) + datetime.timedelta(doy - 1)\n",
    "    return measurement_date\n",
    "\n",
    "def get_zip_avg_lst(hdf_file, zip_codes):\n",
    "    \n",
    "    hdf_ds = gdal.Open(hdf_file)\n",
    "    subdatasets = hdf_ds.GetSubDatasets()\n",
    "    lst_day_sds_name = None\n",
    "    for sds_name, sds_desc in subdatasets:\n",
    "        if 'LST_Day_1km' in sds_name:\n",
    "            lst_day_sds_name = sds_name\n",
    "            break\n",
    "    if lst_day_sds_name is None:\n",
    "        raise Exception('LST_Day_1km subdataset not found.')\n",
    "    lst_day_ds = gdal.Open(lst_day_sds_name)\n",
    "    \n",
    "    lst_day_array = lst_day_ds.ReadAsArray()\n",
    "    nodata_value = lst_day_ds.GetRasterBand(1).GetNoDataValue()\n",
    "    \n",
    "    lst_data = np.where(lst_day_array == nodata_value, np.nan, lst_day_array)\n",
    "    lst_data = lst_data * 0.02  \n",
    "    lst_data = lst_data - 273.15  \n",
    "    \n",
    "    output_raster = 'tmp_lst_day_celsius.tif'\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_ds = driver.Create(\n",
    "        output_raster,\n",
    "        lst_day_ds.RasterXSize,\n",
    "        lst_day_ds.RasterYSize,\n",
    "        1,\n",
    "        gdal.GDT_Float32\n",
    "    )\n",
    "    out_ds.SetGeoTransform(lst_day_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(lst_day_ds.GetProjection())\n",
    "    outband = out_ds.GetRasterBand(1)\n",
    "    outband.WriteArray(lst_data)\n",
    "    outband.SetNoDataValue(np.nan)\n",
    "    outband.FlushCache()\n",
    "    out_ds = None\n",
    "    \n",
    "    reprojected_raster = 'lst_day_epsg4326_celsius.tif'\n",
    "    gdal.Warp(\n",
    "        reprojected_raster,\n",
    "        output_raster,\n",
    "        dstSRS='EPSG:4326',\n",
    "        format='GTiff',\n",
    "        resampleAlg='bilinear',\n",
    "        dstNodata=np.nan\n",
    "    )\n",
    "\n",
    "    zcta_file = ZCTA_SHAPEFILE\n",
    "    zcta_gdf = gpd.read_file(zcta_file)\n",
    "    target_zctas = zcta_gdf[zcta_gdf['ZCTA5CE20'].isin(zip_codes)]\n",
    "    target_zctas = target_zctas.to_crs('EPSG:4326')\n",
    "\n",
    "    stats = zonal_stats(\n",
    "        target_zctas,\n",
    "        reprojected_raster,\n",
    "        stats=['mean'],\n",
    "        nodata=np.nan,\n",
    "        geojson_out=True\n",
    "    )\n",
    "    \n",
    "    zonal_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "    target_zctas = target_zctas.merge(\n",
    "        zonal_gdf[['ZCTA5CE20', 'mean']],\n",
    "        on='ZCTA5CE20',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    target_zctas.rename(columns={'mean': 'avg_LST_C'}, inplace=True)\n",
    "    return target_zctas\n",
    "\n",
    "def concatenate_geojson_files(city_name, directory, output_directory):\n",
    "\n",
    "    file_pattern = os.path.join(directory, f'{city_name}_*.geojson')\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    target_columns = ['ZCTA5CE20', 'avg_LST_C', 'geometry']\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found for {city_name} in {get_fs_name(directory)}.\")\n",
    "        return\n",
    "    \n",
    "    gdfs = []\n",
    "    \n",
    "    for file in tqdm(files, desc=f\"Processing {city_name} files\"):\n",
    "        filename = os.path.basename(file)\n",
    "        date_str = filename.replace(f\"{city_name}_\",\"\").replace(\".geojson\", \"\")\n",
    "        \n",
    "        try:\n",
    "            observation_date = datetime.datetime.strptime(date_str, '%Y%m%d')\n",
    "        except ValueError:\n",
    "            print(f\"Error parsing date from filename {get_fs_name(filename)}. Expected format is {city_name}_YYYYMMDD.geojson\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            gdf = gpd.read_file(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {get_fs_name(filename)}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        available_columns = [col for col in target_columns if col in gdf.columns]\n",
    "        if not available_columns:\n",
    "            print(f\"No target columns found in {get_fs_name(filename)}. Skipping.\")\n",
    "            continue\n",
    "        gdf = gdf[available_columns].copy()\n",
    "        \n",
    "        if 'avg_LST_C' not in gdf.columns:\n",
    "            print(f\"'avg_LST_C' column not found in {get_fs_name(filename)}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        gdf['avg_LST_C'] = gdf['avg_LST_C'].apply(lambda x: np.nan if x is None else x)        \n",
    "        gdf['avg_LST_C'] = pd.to_numeric(gdf['avg_LST_C'], errors='coerce')\n",
    "        gdf.fillna(np.nan, inplace=True)\n",
    "        gdf['observation_date'] = observation_date\n",
    "        gdfs.append(gdf)\n",
    "    \n",
    "    if not gdfs:\n",
    "        print(f\"No valid GeoDataFrames to concatenate for {city_name}.\")\n",
    "        return\n",
    "    \n",
    "    combined_gdf = pd.concat(gdfs, ignore_index=True)\n",
    "    combined_gdf = gpd.GeoDataFrame(combined_gdf, geometry='geometry', crs='EPSG:4326')\n",
    "    output_file = os.path.join(output_directory, f'{city_name}.geojson')\n",
    "    try:\n",
    "        combined_gdf.to_file(output_file, driver='GeoJSON')\n",
    "        print(f\"Combined GeoJSON saved as {get_fs_name(output_file)}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined GeoJSON for {city_name}: {e}\")\n",
    "        \n",
    "        \n",
    "def format_city_name(filename):\n",
    "    return os.path.splitext(os.path.basename(filename))[0].replace(\"_\", \" \").upper()\n",
    "\n",
    "\n",
    "def consolidate_geojson(input_dir, output_file):\n",
    "    file_names = [\"albuquerque.geojson\", \"seattle.geojson\", \"las_vegas.geojson\", \"columbus.geojson\", \"denver.geojson\"]\n",
    "    files = [os.path.join(input_dir, file_name) for file_name in file_names]\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No GeoJSON files found in directory: {get_fs_name(input_dir)}\")\n",
    "        return\n",
    "\n",
    "    gdf_list = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            print(f\"Processing file: {get_fs_name(file)}\")\n",
    "            gdf = gpd.read_file(file)\n",
    "            city_name = format_city_name(file)\n",
    "            gdf['city'] = city_name\n",
    "            gdf_list.append(gdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {get_fs_name(file)}: {e}\")\n",
    "\n",
    "    if not gdf_list:\n",
    "        print(\"No GeoJSON data to consolidate.\")\n",
    "        return\n",
    "\n",
    "    consolidated_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "    consolidated_gdf.to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"Consolidated GeoJSON saved to: {get_fs_name(output_file)}\")\n",
    "\n",
    "def get_zip_avg_vi(hdf_file, zip_codes, vi_type='NDVI'):\n",
    "    hdf_ds = gdal.Open(hdf_file)\n",
    "    subdatasets = hdf_ds.GetSubDatasets()\n",
    "    vi_sds_name = None\n",
    "\n",
    "    for sds_name, sds_desc in subdatasets:\n",
    "        if vi_type in sds_name:\n",
    "            vi_sds_name = sds_name\n",
    "            break\n",
    "\n",
    "    if vi_sds_name is None:\n",
    "        raise Exception(f'{vi_type} subdataset not found.')\n",
    "\n",
    "    vi_ds = gdal.Open(vi_sds_name)\n",
    "\n",
    "    vi_array = vi_ds.ReadAsArray()\n",
    "    nodata_value = vi_ds.GetRasterBand(1).GetNoDataValue()\n",
    "\n",
    "    vi_data = np.where(vi_array == nodata_value, np.nan, vi_array)\n",
    "\n",
    "    if vi_type == 'NDVI' or vi_type == 'EVI':\n",
    "        scale_factor = 0.0001  \n",
    "        vi_data = vi_data * scale_factor  \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Vegetation Index type. Use 'NDVI' or 'EVI'.\")\n",
    "\n",
    "    if vi_type == 'NDVI':\n",
    "        vi_data = np.clip(vi_data, -1, 1)\n",
    "    elif vi_type == 'EVI':\n",
    "        vi_data = np.clip(vi_data, -1, 1)\n",
    "\n",
    "    output_raster = 'tmp_vi.tif'\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_ds = driver.Create(\n",
    "        output_raster,\n",
    "        vi_ds.RasterXSize,\n",
    "        vi_ds.RasterYSize,\n",
    "        1,\n",
    "        gdal.GDT_Float32\n",
    "    )\n",
    "    out_ds.SetGeoTransform(vi_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(vi_ds.GetProjection())\n",
    "    outband = out_ds.GetRasterBand(1)\n",
    "    outband.WriteArray(vi_data)\n",
    "    outband.SetNoDataValue(np.nan)\n",
    "    outband.FlushCache()\n",
    "    out_ds = None  \n",
    "\n",
    "    reprojected_raster = 'vi_epsg4326.tif'\n",
    "    gdal.Warp(\n",
    "        reprojected_raster,\n",
    "        output_raster,\n",
    "        dstSRS='EPSG:4326',\n",
    "        format='GTiff',\n",
    "        resampleAlg='bilinear',\n",
    "        dstNodata=np.nan\n",
    "    )\n",
    "    \n",
    "    zcta_file = ZCTA_SHAPEFILE\n",
    "    zcta_gdf = gpd.read_file(zcta_file)\n",
    "    target_zctas = zcta_gdf[zcta_gdf['ZCTA5CE20'].isin(zip_codes)]\n",
    "    target_zctas = target_zctas.to_crs('EPSG:4326')\n",
    "\n",
    "    stats = zonal_stats(\n",
    "        target_zctas,\n",
    "        reprojected_raster,\n",
    "        stats=['mean'],\n",
    "        nodata=np.nan,\n",
    "        geojson_out=True\n",
    "    )\n",
    "\n",
    "    zonal_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "    \n",
    "    target_zctas = target_zctas.merge(\n",
    "        zonal_gdf[['ZCTA5CE20', 'mean']],\n",
    "        on='ZCTA5CE20',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    target_zctas.rename(columns={'mean': f'avg_{vi_type}'}, inplace=True)\n",
    "    return target_zctas\n",
    "\n",
    "def consolidate_geojson_files(ready_directory, output_file, vi_type='NDVI'):\n",
    "\n",
    "    pattern = os.path.join(ready_directory, f'*_{vi_type}_*.geojson')\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No GeoJSON files found for {vi_type} in directory: {get_fs_name(ready_directory)}\")\n",
    "        return\n",
    "\n",
    "    gdf_list = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            print(f\"Processing file: {get_fs_name(file)}\")\n",
    "            gdf = gpd.read_file(file)\n",
    "\n",
    "            basename = os.path.basename(file)\n",
    "            parts = basename.rsplit('_', 2)  \n",
    "            \n",
    "            if len(parts) != 3:\n",
    "                raise ValueError(f\"Filename {basename} does not match the expected pattern.\")\n",
    "            \n",
    "            city = parts[0].upper()  \n",
    "            date_part = parts[2].split('.')[0]  \n",
    "            \n",
    "            date = datetime.datetime.strptime(date_part, \"%Y%m%d\").date()\n",
    "            \n",
    "            gdf['CITY'] = city\n",
    "            gdf['DATE'] = date\n",
    "            \n",
    "            gdf_list.append(gdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {get_fs_name(file)}: {e}\")\n",
    "\n",
    "    if not gdf_list:\n",
    "        print(\"No GeoJSON data to consolidate.\")\n",
    "        return\n",
    "\n",
    "    consolidated_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "\n",
    "    # output_file = os.path.join(ready_directory, f'cities_consolidated_{vi_type}.geojson')\n",
    "    consolidated_gdf.to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"Consolidated GeoJSON saved to: {get_fs_name(output_file)}\")\n",
    "    \n",
    "def process_lst_data(source_dir, output_file):\n",
    "    directory = f\"{source_dir}/\"\n",
    "    ready_directory = f\"{source_dir}/stage/\"\n",
    "    os.makedirs(ready_directory, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_LST_DATA_DIR, exist_ok=True)\n",
    "    files = os.listdir(directory)\n",
    "    cnt = 0\n",
    "\n",
    "    for modis_file in tqdm(files, desc=\"Processing MODIS LST files\"):\n",
    "        modis_file_full = os.path.join(directory, modis_file)\n",
    "\n",
    "        if os.path.isfile(modis_file_full):\n",
    "            observ_date = datetime.datetime.strftime(get_observation_date(modis_file), \"%Y-%m-%d\")\n",
    "\n",
    "            try:\n",
    "                if \"MOD11A1\" in modis_file and \"h08v05\" in modis_file:\n",
    "                    las_vegas_day_lst = get_zip_avg_lst(modis_file_full, CITIES[\"LAS VEGAS\"][\"zipcodes\"])\n",
    "                    las_vegas_day_lst.to_file(f'''{ready_directory}las_vegas_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "                elif \"MOD11A1\" in modis_file and\"h09v04\" in modis_file:\n",
    "                    seattle_day_lst = get_zip_avg_lst(modis_file_full, CITIES[\"SEATTLE\"][\"zipcodes\"])\n",
    "                    seattle_day_lst.to_file(f'''{ready_directory}seattle_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')                 \n",
    "                elif \"MOD11A1\" in modis_file and \"h09v05\" in modis_file:\n",
    "                    albuquerque_day_lst = get_zip_avg_lst(modis_file_full, CITIES[\"ALBUQUERQUE\"][\"zipcodes\"])\n",
    "                    albuquerque_day_lst.to_file(f'''{ready_directory}albuquerque_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')        \n",
    "                    denver_day_lst = get_zip_avg_lst(modis_file_full, CITIES[\"DENVER\"][\"zipcodes\"])\n",
    "                    denver_day_lst.to_file(f'''{ready_directory}denver_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "                elif \"MOD11A1\" in modis_file and \"h11v05\" in modis_file:\n",
    "                    columbus_day_vi = get_zip_avg_lst(modis_file_full, CITIES[\"COLUMBUS\"][\"zipcodes\"])\n",
    "                    columbus_day_vi.to_file(f'''{ready_directory}columbus_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "                cnt += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {get_fs_name(modis_file)}: {e}\")\n",
    "\n",
    "    concatenate_geojson_files('seattle', ready_directory, ready_directory)\n",
    "    concatenate_geojson_files('las_vegas', ready_directory, ready_directory)\n",
    "    concatenate_geojson_files('albuquerque', ready_directory, ready_directory)\n",
    "    concatenate_geojson_files('denver', ready_directory, ready_directory)\n",
    "    concatenate_geojson_files('columbus', ready_directory, ready_directory)\n",
    "\n",
    "    geojson_file = f\"{ready_directory}/{output_file.split('/')[-1].split('.')[0]}.geojson\"\n",
    "    consolidate_geojson(ready_directory, geojson_file)\n",
    "\n",
    "    print(f\"Processed {cnt} files.\")\n",
    "    output_csv_path = output_file\n",
    "    print(\"Loading GeoJSON file...\")\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    print(\"Dropping geometry column...\")\n",
    "    df = gdf.drop(columns='geometry')\n",
    "    print(\"Saving to CSV...\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"CSV file saved at '{get_fs_name(output_csv_path)}'.\")\n",
    "    \n",
    "    input_csv_path = output_csv_path\n",
    "    output_csv_path = output_csv_path.replace(\"cities.csv\", \"cities_new.csv\")\n",
    "    print(\"Loading CSV file...\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(\"Renaming column 'ZCTA5CE20' to 'zipcode'...\")\n",
    "    df.rename(columns={\"ZCTA5CE20\": \"zipcode\"}, inplace=True)\n",
    "    df_cleaned = df.dropna(subset=[\"avg_LST_C\"])\n",
    "    df_cleaned = df_cleaned.rename(columns={'avg_LST_C': 'avg_lst_c', 'observation_date': 'date'})\n",
    "    print(\"Saving updated CSV...\")\n",
    "    df_cleaned.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Updated CSV saved at '{get_fs_name(output_csv_path)}'.\")\n",
    "    \n",
    "    \n",
    "def process_vi_data(source_dir, output_file):\n",
    "    directory = f\"{source_dir}/\"\n",
    "    ready_directory = f\"{source_dir}/stage/\"\n",
    "    os.makedirs(ready_directory, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_VI_DATA_DIR, exist_ok=True)\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for modis_file in tqdm(files, desc=\"Processing MODIS VI files\"):\n",
    "        modis_file_full = os.path.join(directory, modis_file)\n",
    "        if os.path.isfile(modis_file_full):\n",
    "            observ_date = datetime.datetime.strftime(get_observation_date(modis_file), \"%Y-%m-%d\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                if \"MOD13A2\" in modis_file and \"h08v05\" in modis_file:\n",
    "                    las_vegas_day_vi = get_zip_avg_vi(modis_file_full, CITIES[\"LAS VEGAS\"][\"zipcodes\"], vi_type='NDVI')\n",
    "                    las_vegas_day_vi.to_file(f'''{ready_directory}las_vegas_NDVI_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "                elif \"MOD13A2\" in modis_file and \"h09v04\" in modis_file:\n",
    "                    seattle_day_vi = get_zip_avg_vi(modis_file_full, CITIES[\"SEATTLE\"][\"zipcodes\"], vi_type='NDVI')\n",
    "                    seattle_day_vi.to_file(f'''{ready_directory}seattle_NDVI_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')                 \n",
    "                elif \"MOD13A2\" in modis_file and \"h09v05\" in modis_file:\n",
    "                    albuquerque_day_vi = get_zip_avg_vi(modis_file_full, CITIES[\"ALBUQUERQUE\"][\"zipcodes\"], vi_type='NDVI')\n",
    "                    albuquerque_day_vi.to_file(f'''{ready_directory}albuquerque_NDVI_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')        \n",
    "                    denver_day_vi = get_zip_avg_vi(modis_file_full, CITIES[\"DENVER\"][\"zipcodes\"], vi_type='NDVI')\n",
    "                    denver_day_vi.to_file(f'''{ready_directory}denver_NDVI_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "                elif \"MOD13A2\" in modis_file and \"h11v05\" in modis_file:\n",
    "                    columbus_day_vi = get_zip_avg_vi(modis_file_full, CITIES[\"COLUMBUS\"][\"zipcodes\"], vi_type='NDVI')\n",
    "                    columbus_day_vi.to_file(f'''{ready_directory}columbus_NDVI_{observ_date.replace('-', '')}.geojson''', driver='GeoJSON')\n",
    "\n",
    "                cnt += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {get_fs_name(modis_file)}: {e}\")\n",
    "\n",
    "    print(f\"Processed {cnt} files.\")\n",
    "\n",
    "    geojson_file = f\"{ready_directory}/{output_file.split('/')[-1].split('.')[0]}.geojson\"\n",
    "    consolidate_geojson_files(ready_directory, geojson_file, vi_type='NDVI')\n",
    "    \n",
    "    output_csv_path = output_file\n",
    "    print(\"Loading GeoJSON file...\")\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    print(\"Dropping geometry column...\")\n",
    "    df = gdf.drop(columns='geometry')\n",
    "    df.rename(columns={\"ZCTA5CE20\": \"zipcode\"}, inplace=True)\n",
    "    df.rename(columns={\"DATE\": \"date\"}, inplace=True)\n",
    "    df.rename(columns={\"CITY\": \"city\"}, inplace=True)\n",
    "    df.rename(columns={\"avg_NDVI\": \"avg_ndvi\"}, inplace=True)\n",
    "    df = df[[\"zipcode\", \"date\", \"avg_ndvi\", \"city\"]]\n",
    "    df.columns = df.columns.str.strip()\n",
    "    date_range = pd.date_range(start=\"2022-01-01\", end=\"2023-12-31\").strftime('%Y-%m-%d')\n",
    "    unique_zipcodes = df['zipcode'].unique()\n",
    "    full_combinations = pd.DataFrame(\n",
    "        list(pd.MultiIndex.from_product([unique_zipcodes, date_range], names=[\"zipcode\", \"date\"])),\n",
    "        columns=[\"zipcode\", \"date\"]\n",
    "    )\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    full_combinations['date'] = full_combinations['date'].astype(str)\n",
    "    result_df = pd.merge(\n",
    "        full_combinations,\n",
    "        df,\n",
    "        how=\"left\",\n",
    "        on=[\"zipcode\", \"date\"]\n",
    "    )\n",
    "    result_df['avg_ndvi'] = result_df.groupby('zipcode')['avg_ndvi'].ffill()\n",
    "    result_df['city'] = result_df.groupby('zipcode')['city'].ffill()\n",
    "    result_df.dropna(subset=['avg_ndvi', 'city'], inplace=True)\n",
    "    result_df.dropna(inplace=True)\n",
    "    print(\"Saving to CSV...\")\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\"\"CSV file saved at '{get_fs_name(output_csv_path)}'.\"\"\")\n",
    "\n",
    "def combine_daymet_data(stage_dir):\n",
    "    meteo_sat_dir = PROCESSED_DAYMET_DATA_DIR\n",
    "    output_csv_path = os.path.join(meteo_sat_dir, \"combined_meteo_sat_data.csv\")\n",
    "    city_mapping = {\n",
    "        \"ALBUQUERQUE\": \"ALBUQUERQUE\",\n",
    "        \"COLUMBUS\": \"COLUMBUS\",\n",
    "        \"DENVER\": \"DENVER\",\n",
    "        \"LAS_VEGAS\": \"LAS_VEGAS\",\n",
    "        \"SEATTLE\": \"SEATTLE\",\n",
    "    }\n",
    "\n",
    "    dataframes = []\n",
    "    \n",
    "    print(\"Processing files...\")\n",
    "    for file_name in os.listdir(stage_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            for city_key, city_name in city_mapping.items():\n",
    "                if city_key in file_name:\n",
    "                    file_path = os.path.join(stage_dir, file_name)\n",
    "                    variable = file_name.split(\"_avg_\")[-1].replace(\".csv\", \"\")\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df.rename(columns={df.columns[-1]: variable}, inplace=True)\n",
    "                    df[\"city\"] = city_name\n",
    "                    dataframes.append(df)\n",
    "                    break\n",
    "    \n",
    "    print(\"Combining data...\")\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        combined_df = combined_df.pivot_table(\n",
    "            index=[\"time\", \"zipcode\", \"city\"], \n",
    "            columns=[], \n",
    "            values=list(combined_df.columns.difference([\"time\", \"zipcode\", \"city\"]))\n",
    "        ).reset_index()\n",
    "        print(\"Saving combined data to CSV...\")\n",
    "        combined_df['time'] = pd.to_datetime(combined_df['time']).dt.date  \n",
    "        combined_df.rename(columns={'time': 'date'}, inplace=True) \n",
    "        combined_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\"\"Combined data saved to '{get_fs_name(output_csv_path)}'.\"\"\")\n",
    "    else:\n",
    "        print(\"No files to process.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def process_daymet_data(source_dir):\n",
    "    stage_directory = f\"{source_dir}/stage/\"\n",
    "    os.makedirs(stage_directory, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DAYMET_DATA_DIR, exist_ok=True)\n",
    "    variables = ['dayl', 'prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp']\n",
    "    data_dir = source_dir\n",
    "    zcta_file = ZCTA_SHAPEFILE\n",
    "    print(\"Loading ZIP code shapefile...\")\n",
    "    zcta_gdf = gpd.read_file(zcta_file)\n",
    "    for city, conf in CITIES.items():\n",
    "        print(f\"Processing city: {city}\")\n",
    "        zip_codes = conf.get(\"zipcodes\",[])\n",
    "        tiles = conf.get(\"daymet\",[])\n",
    "        if not zip_codes:\n",
    "            print(f\"No zip codes found for city: {city}\")\n",
    "            continue\n",
    "        city_zcta_gdf = zcta_gdf[zcta_gdf['ZCTA5CE20'].isin(zip_codes)].copy()\n",
    "        if city_zcta_gdf.empty:\n",
    "            print(f\"No matching ZIP codes found in shapefile for city: {city}\")\n",
    "            continue\n",
    "        if city_zcta_gdf.crs is None:\n",
    "            print(f\"ZIP code shapefile for city {city} has no CRS. Skipping.\")\n",
    "            continue\n",
    "        city_variable_dfs = {var: [] for var in variables}\n",
    "        for tile in tiles:\n",
    "            print(f\"  Processing tile: {tile}\")\n",
    "            # Iterate through each variable\n",
    "            for var in variables:\n",
    "                # Find files for this tile and variable\n",
    "                var_pattern = os.path.join(data_dir, f\"{tile}_*_{var}.nc\")\n",
    "                var_files = glob.glob(var_pattern)\n",
    "                if not var_files:\n",
    "                    print(f\"    No files found for variable '{var}' in tile '{tile}'.\")\n",
    "                    continue\n",
    "                for var_file in var_files:\n",
    "                    print(f\"    Processing file: {get_fs_name(var_file)}\")\n",
    "                    try:\n",
    "                        dataset = xr.open_dataset(var_file)\n",
    "                        if var not in dataset:\n",
    "                            print(f\"      Variable '{var}' not found in file '{get_fs_name(var_file)}'. Skipping.\")\n",
    "                            continue\n",
    "                        variable_data = dataset[var]\n",
    "                        lat = dataset['lat'].values\n",
    "                        lon = dataset['lon'].values\n",
    "                        x_coords = dataset['x'].values\n",
    "                        y_coords = dataset['y'].values\n",
    "                        df = variable_data.to_dataframe().reset_index()\n",
    "                        df['y_index'] = df['y'].apply(lambda y_val: np.abs(y_coords - y_val).argmin())\n",
    "                        df['x_index'] = df['x'].apply(lambda x_val: np.abs(x_coords - x_val).argmin())\n",
    "                        df['lat'] = df['y_index'].apply(lambda y_idx: lat[y_idx, 0])\n",
    "                        df['lon'] = df['x_index'].apply(lambda x_idx: lon[0, x_idx])\n",
    "                        df = df.dropna(subset=[var])\n",
    "                        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=\"EPSG:4326\")\n",
    "                        gdf = gdf.to_crs(city_zcta_gdf.crs)\n",
    "                        joined_gdf = gpd.sjoin(gdf, city_zcta_gdf, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "                        if joined_gdf.empty:\n",
    "                            print(f\"      No data points found within ZIP codes for file '{get_fs_name(var_file)}'.\")\n",
    "                            continue\n",
    "                        \n",
    "                        grouped = (\n",
    "                            joined_gdf.groupby(['time', 'ZCTA5CE20'])[var]\n",
    "                            .mean()\n",
    "                            .reset_index()\n",
    "                            .rename(columns={'ZCTA5CE20': 'zipcode', var: f'avg_{var}'})\n",
    "                        )\n",
    "                        \n",
    "                        city_variable_dfs[var].append(grouped)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      Error processing file '{get_fs_name(var_file)}': {e}\")\n",
    "        \n",
    "        for var in variables:\n",
    "            if not city_variable_dfs[var]:\n",
    "                print(f\"  No data collected for variable '{var}' in city '{city}'. Skipping.\")\n",
    "                continue\n",
    "            print(f\"  Aggregating data for variable '{var}'\")\n",
    "            combined_df = pd.concat(city_variable_dfs[var], ignore_index=True)\n",
    "            aggregated_df = combined_df.groupby(['time', 'zipcode'])[f'avg_{var}'].mean().reset_index()\n",
    "            output_filename = f\"{city}_zipcode_avg_{var}.csv\"\n",
    "            aggregated_df.to_csv(f\"{stage_directory}/{output_filename}\", index=False)\n",
    "            print(f\"    Saved aggregated data to '{get_fs_name(output_filename)}'\")\n",
    "        print(f\"Finished processing city: {city}\")\n",
    "    print(\"All cities processed.\")\n",
    "    combine_daymet_data(stage_directory)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def process_population_data():\n",
    "    os.makedirs(PROCESSED_CENSUS_DATA_DIR, exist_ok=True)\n",
    "    population_file = f\"{CENSUS_DATA_DIR}/DECENNIALDHC2020.P1-Data.csv\"\n",
    "    zcta_shapefile_path = ZCTA_SHAPEFILE\n",
    "    \n",
    "    output_dir = PROCESSED_CENSUS_DATA_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_csv_path = os.path.join(output_dir, 'zipcode_population_density_filtered.csv')\n",
    "    \n",
    "    target_zip_codes = [zip_code for city, conf in CITIES.items() for zip_code in conf[\"zipcodes\"]]    \n",
    "    print(\"Loading population data...\")\n",
    "    population_df = pd.read_csv(population_file)\n",
    "    population_df['ZIP'] = population_df['NAME'].str.replace(\"ZCTA5 \", \"\").str.zfill(5)\n",
    "    population_df['Population'] = pd.to_numeric(population_df['P1_001N'], errors='coerce')\n",
    "    population_df = population_df[population_df['ZIP'].isin(target_zip_codes)]\n",
    "    print(f\"Filtered population data rows: {len(population_df)}\")\n",
    "    print(\"Loading ZCTA shapefile...\")\n",
    "    zcta_gdf = gpd.read_file(zcta_shapefile_path)\n",
    "    zcta_gdf['ZIP'] = zcta_gdf['ZCTA5CE20'].astype(str).str.zfill(5)\n",
    "    zcta_gdf = zcta_gdf[zcta_gdf['ZIP'].isin(target_zip_codes)]\n",
    "    print(f\"Filtered ZCTA rows: {len(zcta_gdf)}\")\n",
    "    common_zips = set(population_df['ZIP']) & set(zcta_gdf['ZIP'])\n",
    "    print(f\"Common ZIP codes: {len(common_zips)}\")\n",
    "    \n",
    "    if not common_zips:\n",
    "        print(\"No common ZIP codes found. Please check the data formats.\")\n",
    "        exit()\n",
    "\n",
    "    zcta_gdf['Area_sqkm'] = zcta_gdf.geometry.to_crs(epsg=3395).area / 1e6\n",
    "    print(\"Merging population data with ZCTA geometry...\")\n",
    "    merged_gdf = zcta_gdf.merge(population_df, on='ZIP', how='inner')\n",
    "    print(f\"Merged rows: {len(merged_gdf)}\")\n",
    "    if merged_gdf.empty:\n",
    "        print(\"Merged dataset is empty. Check for ZIP code mismatches.\")\n",
    "        exit()\n",
    "    merged_gdf['Population_Density'] = merged_gdf['Population'] / merged_gdf['Area_sqkm']\n",
    "    merged_gdf.rename(columns={\"ZIP\": \"zipcode\"}, inplace=True)\n",
    "    output_df = merged_gdf[['zipcode', 'Population', 'Area_sqkm', 'Population_Density']]\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Filtered population density data saved to '{get_fs_name(output_csv_path)}'.\")\n",
    "    \n",
    "def process_meteostat_data():\n",
    "    os.makedirs(PROCESSED_METEOSTAT_DATA_DIR, exist_ok=True)\n",
    "    meteo_dir = METEOSTAT_DATA_DIR\n",
    "    output_csv_path = os.path.join(PROCESSED_METEOSTAT_DATA_DIR, \"combined_meteo_data_with_city.csv\")\n",
    "\n",
    "    city_mapping = {\n",
    "        \"albuquerque\": \"ALBUQUERQUE\",\n",
    "        \"columbus\": \"COLUMBUS\",\n",
    "        \"denver\": \"DENVER\",\n",
    "        \"las_vegas\": \"LAS VEGAS\",\n",
    "        \"los_angeles\": \"LOS ANGELES\",\n",
    "        \"portland\": \"PORTLAND\",\n",
    "        \"seattle\": \"SEATTLE\",\n",
    "    }\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    print(\"Combining CSV files...\")\n",
    "    for file_name in os.listdir(meteo_dir):\n",
    "        if file_name.endswith(\".csv\") and file_name != \"combined_meteo_data.csv\":\n",
    "            for city_key, city_name in city_mapping.items():\n",
    "                if city_key in file_name:\n",
    "                    file_path = os.path.join(meteo_dir, file_name)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df[\"city\"] = city_name\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                    break\n",
    "    print(\"Saving combined data to CSV...\")\n",
    "    combined_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Combined data saved to '{get_fs_name(output_csv_path)}'.\")\n",
    "    \n",
    "def process_lc_data():\n",
    "    zcta_shapefile_path = ZCTA_SHAPEFILE\n",
    "    land_cover_raster_path = f\"{LC_DATA_DIR}/LndCov.tif\"\n",
    "    output_dir = PROCESSED_LC_DATA_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    land_cover_mapping = {\n",
    "        11: 'Open Water',\n",
    "        21: 'Developed, Open Space',\n",
    "        22: 'Developed, Low Intensity',\n",
    "        23: 'Developed, Medium Intensity',\n",
    "        24: 'Developed, High Intensity',\n",
    "        31: 'Barren Land',\n",
    "        41: 'Deciduous Forest',\n",
    "        42: 'Evergreen Forest',\n",
    "        43: 'Mixed Forest',\n",
    "        52: 'Shrub/Scrub',\n",
    "        71: 'Grassland/Herbaceous',\n",
    "        81: 'Pasture/Hay',\n",
    "        82: 'Cultivated Crops',\n",
    "        90: 'Woody Wetlands',\n",
    "        95: 'Emergent Herbaceous Wetlands'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(zcta_shapefile_path):\n",
    "        raise FileNotFoundError(f\"ZCTA shapefile not found at {zcta_shapefile_path}\")\n",
    "    if not os.path.exists(land_cover_raster_path):\n",
    "        raise FileNotFoundError(f\"Land cover raster not found at {land_cover_raster_path}\")\n",
    "        \n",
    "    print(\"Loading ZIP code shapefile...\")\n",
    "    zcta_gdf = gpd.read_file(zcta_shapefile_path)\n",
    "    zcta_gdf['ZCTA5CE20'] = zcta_gdf['ZCTA5CE20'].astype(str).str.zfill(5)\n",
    "    \n",
    "    print(\"Original ZCTA CRS:\", zcta_gdf.crs)\n",
    "\n",
    "    if zcta_gdf.crs is None:\n",
    "        zcta_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    with rasterio.open(land_cover_raster_path) as src:\n",
    "        raster_crs = src.crs\n",
    "        print(\"Raster CRS:\", raster_crs)\n",
    "    \n",
    "    if raster_crs != zcta_gdf.crs:\n",
    "        print(\"CRS mismatch detected. Reprojecting shapefile to raster CRS...\")\n",
    "        zcta_gdf = zcta_gdf.to_crs(raster_crs)\n",
    "        print(\"Reprojection complete.\")\n",
    "    \n",
    "    print(\"Filtering ZCTA GeoDataFrame for target ZIP codes...\")\n",
    "    target_zip_codes = [zip_code for city, conf in CITIES.items() for zip_code in conf[\"zipcodes\"]]\n",
    "    zcta_filtered_gdf = zcta_gdf[zcta_gdf['ZCTA5CE20'].isin(target_zip_codes)]\n",
    "    print(f\"Filtered ZIP codes count: {len(zcta_filtered_gdf)}\")\n",
    " \n",
    "    all_land_cover_classes = [land_cover_mapping.get(code, f\"Class_{code}\") for code in land_cover_mapping.keys()]\n",
    "    \n",
    "    results = []\n",
    "    print(\"Calculating land cover statistics...\")\n",
    "    for _, row in zcta_filtered_gdf.iterrows():\n",
    "        zipcode = row['ZCTA5CE20']\n",
    "        geometry = row['geometry']\n",
    "    \n",
    "        try:\n",
    "            stats = zonal_stats(\n",
    "                geometry,\n",
    "                land_cover_raster_path,\n",
    "                stats=None,\n",
    "                categorical=True,\n",
    "                nodata=250,\n",
    "                all_touched=False\n",
    "            )\n",
    "            if not stats:\n",
    "                continue\n",
    "            class_counts = stats[0]\n",
    "            class_counts.pop(250, None)  \n",
    "            total_pixels = sum(class_counts.values())\n",
    "            class_percentages = {\n",
    "                land_cover_mapping.get(class_code, f\"Class_{class_code}\"): round((count / total_pixels) * 100, 2)\n",
    "                for class_code, count in class_counts.items()\n",
    "            }\n",
    "            for land_cover_class in all_land_cover_classes:\n",
    "                if land_cover_class not in class_percentages:\n",
    "                    class_percentages[land_cover_class] = 0.0\n",
    "            class_percentages['zipcode'] = zipcode\n",
    "            results.append(class_percentages)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ZIP code {zipcode}: {e}\")\n",
    "    output_csv_path = os.path.join(output_dir, 'zipcode_land_cover_percentages.csv')\n",
    "    land_cover_df = pd.DataFrame(results)\n",
    "    cols = ['zipcode'] + all_land_cover_classes\n",
    "    land_cover_df = land_cover_df.reindex(columns=cols, fill_value=0.0)\n",
    "    \n",
    "    land_cover_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to '{get_fs_name(output_csv_path)}'.\")\n",
    "    \n",
    "def process_fis_data():\n",
    "    def calculate_isa(zcta, raster_path, pixel_area_m2):\n",
    "        try:\n",
    "            geometry = [zcta['geometry'].__geo_interface__]\n",
    "            stats = zonal_stats(\n",
    "                geometry,\n",
    "                raster_path,\n",
    "                stats=[\"sum\", \"count\"],\n",
    "                nodata=250.0,  \n",
    "                all_touched=False,  \n",
    "                raster_out=False\n",
    "            )\n",
    "            \n",
    "            if not stats:\n",
    "                return {'zipcode': zcta['ZCTA5CE20'], 'total_impervious_km2': 0, 'percent_impervious': 0}\n",
    "            \n",
    "            impervious_sum = stats[0]['sum']  \n",
    "            count = stats[0]['count']  \n",
    "            if count == 0:\n",
    "                return {'zipcode': zcta['ZCTA5CE20'], 'total_impervious_km2': 0, 'percent_impervious': 0}\n",
    "            mean_impervious = impervious_sum / count  # In percentage\n",
    "            \n",
    "            if mean_impervious > 100:\n",
    "                mean_impervious = (mean_impervious / 255) * 100  # Convert to percentage\n",
    "            total_impervious_km2 = (mean_impervious / 100) * (pixel_area_m2 * count) / 1e6\n",
    "            \n",
    "            percent_impervious = mean_impervious\n",
    "            \n",
    "            percent_impervious = min(percent_impervious, 100.0)\n",
    "            \n",
    "            return {\n",
    "                'zipcode': zcta['ZCTA5CE20'],\n",
    "                'total_impervious_km2': round(total_impervious_km2, 4),\n",
    "                'percent_impervious': round(percent_impervious, 2)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ZIP code {zcta['ZCTA5CE20']}: {e}\")\n",
    "            return {'zipcode': zcta['ZCTA5CE20'], 'total_impervious_km2': np.nan, 'percent_impervious': np.nan}\n",
    "\n",
    "\n",
    "    nlcd_raster_path = f'{FIS_DATA_DIR}/FctImp.tif'\n",
    "    zcta_shapefile_path = ZCTA_SHAPEFILE\n",
    "    \n",
    "    output_dir = PROCESSED_FIS_DATA_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading ZIP code shapefile...\")\n",
    "    zcta_gdf = gpd.read_file(zcta_shapefile_path)\n",
    "    \n",
    "    zcta_gdf['ZCTA5CE20'] = zcta_gdf['ZCTA5CE20'].astype(str).str.zfill(5)\n",
    "    \n",
    "    with rasterio.open(nlcd_raster_path) as src:\n",
    "        nlcd_crs = src.crs\n",
    "        pixel_width, pixel_height = src.res \n",
    "        raster_transform = src.transform\n",
    "    \n",
    "    zcta_gdf = zcta_gdf.to_crs(nlcd_crs)\n",
    "    \n",
    "    target_zip_codes = [zip_code for city, conf in CITIES.items() for zip_code in conf[\"zipcodes\"]]\n",
    "    zcta_filtered_gdf = zcta_gdf[zcta_gdf['ZCTA5CE20'].isin(target_zip_codes)].copy()\n",
    "    \n",
    "    if zcta_filtered_gdf.empty:\n",
    "        print(\"No matching ZIP codes found in the shapefile for the specified cities.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(f\"Filtered to {len(zcta_filtered_gdf)} ZIP codes for the specified cities.\")\n",
    "    \n",
    "    pixel_area_m2 = pixel_width * pixel_height\n",
    "    isa_metrics = []\n",
    "    \n",
    "    print(\"Calculating ISA metrics for each ZIP code...\")\n",
    "    for idx, zcta in zcta_filtered_gdf.iterrows():\n",
    "        metrics = calculate_isa(zcta, nlcd_raster_path, pixel_area_m2)\n",
    "        isa_metrics.append(metrics)\n",
    "\n",
    "    isa_df = pd.DataFrame(isa_metrics)\n",
    "\n",
    "    output_csv_path = os.path.join(output_dir, 'zipcode_impervious_surface_area_corrected.csv')\n",
    "    isa_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Corrected ISA metrics saved to '{get_fs_name(output_csv_path)}'\")\n",
    "    \n",
    "def create_final_dataset():\n",
    "    os.makedirs(FINAL_DATA_DIR, exist_ok=True)\n",
    "    lst_file = os.path.join(PROCESSED_LST_DATA_DIR, \"cities_lst.csv\")\n",
    "    meteo_per_city_file = os.path.join(PROCESSED_METEOSTAT_DATA_DIR, \"combined_meteo_data_with_city.csv\")\n",
    "    meteo_sat_file = os.path.join(PROCESSED_DAYMET_DATA_DIR, \"combined_meteo_sat_data.csv\")\n",
    "    population_file = os.path.join(PROCESSED_CENSUS_DATA_DIR, \"zipcode_population_density_filtered.csv\")\n",
    "    vegetation_file = os.path.join(PROCESSED_VI_DATA_DIR, \"cities_vi.csv\")\n",
    "    impervious_surface_file = os.path.join(PROCESSED_FIS_DATA_DIR, \"zipcode_impervious_surface_area_corrected.csv\")\n",
    "    land_cover_file = os.path.join(PROCESSED_LC_DATA_DIR, \"zipcode_land_cover_percentages.csv\")\n",
    "    output_csv_path = \"data/processed/uhi.csv\"\n",
    "    print(\"Loading files...\")\n",
    "    lst_df = pd.read_csv(lst_file)\n",
    "    meteo_sat_df = pd.read_csv(meteo_sat_file)\n",
    "    vegetation_df = pd.read_csv(vegetation_file)\n",
    "    meteo_per_city_df = pd.read_csv(meteo_per_city_file)\n",
    "    population_df = pd.read_csv(population_file)\n",
    "    impervious_surface_df = pd.read_csv(impervious_surface_file)\n",
    "    land_cover_df = pd.read_csv(land_cover_file)\n",
    "    print(\"Selecting required columns from meteo_per_city...\")\n",
    "    meteo_per_city_df = meteo_per_city_df[['date', 'city', 'wdir', 'wspd', 'pres']]\n",
    "    print(f\"Merging lst_df with meteo_sat_df ...\")\n",
    "    df_merged = lst_df.merge(meteo_sat_df, on=[\"zipcode\", \"date\"], how=\"inner\")\n",
    "    df_merged = df_merged.merge(vegetation_df, on=[\"zipcode\", \"date\"], how=\"inner\")\n",
    "    df_merged = df_merged.merge(meteo_per_city_df, on=[\"city\", \"date\"], how=\"inner\")\n",
    "    population_df.rename(columns={\"ZIP\": \"zipcode\"}, inplace=True)\n",
    "    df_merged = df_merged.merge(population_df[[\"zipcode\", \"Population_Density\"]], on=[\"zipcode\"], how=\"inner\")\n",
    "    df_merged = df_merged.merge(impervious_surface_df[[\"zipcode\", \"percent_impervious\"]], on=[\"zipcode\"], how=\"inner\")\n",
    "    df_merged = df_merged.merge(land_cover_df, on=[\"zipcode\"], how=\"inner\")\n",
    "    df_merged['min_lst'] = df_merged.groupby(['date', 'city'])['avg_lst_c'].transform('min')\n",
    "    df_merged['lst_diff'] = df_merged['avg_lst_c'] - df_merged['min_lst']\n",
    "    df_merged['uhi_observed'] = df_merged['lst_diff'].apply(lambda x: 'Y' if x > 5 else 'N')\n",
    "    df_merged.drop(['min_lst', 'lst_diff', 'city_x', 'city_y', 'srad', 'swe'], axis=1, inplace=True)\n",
    "    df_merged.rename(columns={\n",
    "        \"dayl\": \"Day_length_sec\",\n",
    "        \"prcp\": \"precipitation_mm\",\n",
    "        \"tmax\": \"Max_air_temp_c\",\n",
    "        \"tmin\": \"Min_air_temp_c\",\n",
    "        \"vp\": \"Water_vapor_pressure_pa\",\n",
    "        \"wdir\": \"wind_dir_degrees\",\n",
    "        \"wspd\": \"wind_speed_kmh\",\n",
    "        \"pres\": \"atm_pressure_hpa\",\n",
    "        \"Open Water\": \"open_water_pct\",\n",
    "        \"Developed, Open Space\": \"developed_open_space_pct\",\n",
    "        \"Developed, Low Intensity\": \"Developed_Low_Intensity_pct\",\n",
    "        \"Developed, Medium Intensity\": \"Developed_Medium_Intensity_pct\",\n",
    "        \"Developed, High Intensity\": \"Developed_High_Intensity_pct\",\n",
    "        \"Barren Land\": \"Barren_Land_pct\",\n",
    "        \"Deciduous Forest\": \"Deciduous_Forest_pct\",\n",
    "        \"Evergreen Forest\": \"Evergreen_Forest_pct\",\n",
    "        \"Mixed Forest\": \"Mixed_Forest_pct\",\n",
    "        \"Shrub/Scrub\": \"Shrub_Scrub_pct\",\n",
    "        \"Grassland/Herbaceous\": \"Grassland_Herbaceous_pct\",\n",
    "        \"Pasture/Hay\": \"Pasture_Hay_pct\",\n",
    "        \"Cultivated Crops\": \"Cultivated_Crops\",\n",
    "        \"Woody Wetlands\": \"Woody_Wetlands_pct\",\n",
    "        \"Emergent Herbaceous Wetlands\": \"Emergent_Herbaceous_Wetlands_pct\"\n",
    "    }, inplace=True)\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\"\"Dataset saved to '{get_fs_name(output_csv_path)}'\"\"\")\n",
    "\n"
   ],
   "id": "843612c0ec83f919",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Downloading and Processing Data",
   "id": "d47b466d890913b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1. Download a geographical shapefile with ZIP code service areas boundaries\n",
    "\n",
    "The TIGER/Line Shapefiles are a core geographic product from the Census Bureau that contain extracts of geographic and cartographic information from the MAF/TIGER database. They  are approximate representations of ZIP Code service areas created by the Census Bureau to present statistical data. \n",
    "\n"
   ],
   "id": "e14b5bf9a5ed8bc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T21:23:43.361915Z",
     "start_time": "2024-11-28T21:23:24.652015Z"
    }
   },
   "cell_type": "code",
   "source": "download_zipcode_data()",
   "id": "9f476b6b24271f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www2.census.gov/geo/tiger/TIGER2024/ZCTA520/tl_2024_us_zcta520.zip...\n",
      "Extracting files...\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2. Download MODIS Land Surface Temperature data\n",
    "\n",
    "The MODIS MOD11A1 product is a daily, 1km resolution dataset from the Terra satellite's Moderate Resolution Imaging Spectroradiometer (MODIS) that provides Land Surface Temperature (LST) and Emissivity data for each pixel on Earth, essentially giving a detailed picture of the surface temperature across the globe with high spatial accuracy; it includes quality control flags to indicate data reliability and is commonly used in studies related to climate, land use, and environmental monitoring.\n",
    "\n",
    "- Each pixel in the grid represents the land surface temperature at that location. \n",
    "- Quality control flags indicate:\n",
    "  - Whether the pixel data is reliable (clear sky).\n",
    "  - Whether the pixel data is potentially affected by clouds.\n",
    "- Multiple observations** may be averaged for a single pixel, especially at higher latitudes where the satellite may pass over the same area multiple times.\n",
    "\n",
    "Format: HDFv4\n",
    "\n",
    "To cover the cities used in our modeling we need to download 16GB of data.\n",
    "It takes long time to complete the download, so I use earlier downloaded date and do not repeat it here.\n",
    "\n"
   ],
   "id": "3bd4b39b253da6bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "download_lst_data()",
   "id": "60a47eb8fd0db99a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3. Download MODIS Normalized Difference Vegetation Index data\n",
    "\n",
    "MODIS MOD13A2 is a NASA data product derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) that provides a 16-day global composite of vegetation indices, primarily the Normalized Difference Vegetation Index (NDVI) at a 1km spatial resolution, allowing researchers to monitor vegetation health and changes over time across the globe; it uses the best available pixel value from multiple acquisitions within the 16-day period based on factors like cloud cover and view angle.\n",
    "\n",
    "The Normalized Difference Vegetation Index (NDVI) calculated from the red and near-infrared bands of the MODIS sensor, allowing for better vegetation monitoring in areas with high biomass.\n",
    "\n",
    "Format: HDFv4\n",
    "\n",
    "To cover the cities used in our modeling we need to download more than 3GB of data.\n",
    "It takes long time to complete the download, so I use earlier downloaded date and do not repeat it here.\n",
    "\n"
   ],
   "id": "e194d660024b8288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "download_vi_data()",
   "id": "b50a9bafc8cdaf0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4. Download satellite meteo data\n",
    "\n",
    "DAYMET is a research product that provides gridded estimates of daily weather parameters.\n",
    "It uses algorithms and software to interpolate and extrapolate from daily meteorological observations to produce gridded estimates of daily weather parameters.\n",
    "\n",
    "DAYMET data products are used in many Earth science, natural resource, biodiversity, and agricultural research areas.\n",
    "\n",
    "Format: netCDF/Network Common Data\n",
    "\n",
    "To cover the cities used in our modeling we need to download 2GB of data.\n",
    "It takes long time to complete the download, so I use earlier downloaded date and do not repeat it here.\n",
    "\n"
   ],
   "id": "9350626bf042d0ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare fo daymet data download\n",
    "os.makedirs(DAYMET_DATA_DIR, exist_ok=True)\n",
    "os.environ[\"DAYMET_DATA_DIR\"] = DAYMET_DATA_DIR"
   ],
   "id": "a32c3ead0643d565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%sh\n",
    "# download daymet data\n",
    "\n",
    "years=(2022 2023)\n",
    "variables=(\"dayl\" \"prcp\" \"srad\" \"swe\" \"tmax\" \"tmin\" \"vp\")\n",
    "tiles=(11193 11197 11373 11558 11569 11749 12269)\n",
    "\n",
    "# Iterate through both lists\n",
    "for year in \"${years[@]}\"; do\n",
    "  for variable in \"${variables[@]}\"; do\n",
    "    for tile in \"${tiles[@]}\"; do\n",
    "      wget -q --show-progress --progress=bar:force --limit-rate=3m https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/2129/tiles/${year}/${tile}_${year}/${variable}.nc -O $DAYMET_DATA_DIR/${tile}_${year}_${variable}.nc\n",
    "    done\n",
    "  done\n",
    "done\n",
    "\n"
   ],
   "id": "cb7645bce919668e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "108d8b2696cdc117"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5. Download satellite Land Cover data\n",
    "\n",
    "MRLC Land Cover product depicts the predominant thematic land cover class within the mapping year with respect to broad categories of artificial or natural surface cover.\n",
    "\n",
    "Format: GTiff/GeoTIFF\n"
   ],
   "id": "c2c38357a5f4209c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T21:53:21.626908Z",
     "start_time": "2024-11-28T21:53:21.619281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare for Land Cover data download\n",
    "os.makedirs(LC_DATA_DIR, exist_ok=True)\n",
    "os.environ[\"LC_DATA_DIR\"] = LC_DATA_DIR"
   ],
   "id": "95ffb239500bcf4",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T00:43:34.036717Z",
     "start_time": "2024-11-29T00:42:58.106842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%sh\n",
    "echo Start download\n",
    "wget -q  https://s3-us-west-2.amazonaws.com/mrlc/Annual_NLCD_LndCov_2023_CU_C1V0.tif -O $LC_DATA_DIR/LndCov.tif\n",
    "echo Download complete"
   ],
   "id": "7db4476925535323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download\n",
      "Download complete\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6. Download satellite Fractional Impervious Surface data\n",
    "\n",
    "The Annual National Land Cover Database (NLCD) impervious surface product provides the fractional or percent surface area of a 30-meter map pixel that is covered with processed materials or structures (pavement, concrete, rooftops, and other constructed materials) that generate surface runoff.\n",
    "\n",
    "Format: GTiff/GeoTIFF"
   ],
   "id": "7e57c2b07851f610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare for Fractional Impervious Surface data download\n",
    "os.makedirs(FIS_DATA_DIR, exist_ok=True)\n",
    "os.environ[\"FIS_DATA_DIR\"] = FIS_DATA_DIR"
   ],
   "id": "a2f297e293426609"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T00:48:46.692557Z",
     "start_time": "2024-11-29T00:48:21.203333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%sh\n",
    "echo Start download\n",
    "wget -q https://s3-us-west-2.amazonaws.com/mrlc/Annual_NLCD_FctImp_2023_CU_C1V0.tif -O $FIS_DATA_DIR/FctImp.tif\n",
    "echo Download complete"
   ],
   "id": "e3023fc84ded999e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download\n",
      "Download complete\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a8a24249daa302f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7. Download population density data\n",
    "\n",
    "US Census Bureau zip code total population 2020\n",
    "\n",
    "Format: CSV"
   ],
   "id": "c3fe6310e7bb605a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:01:28.331884Z",
     "start_time": "2024-11-28T22:01:28.014462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%sh\n",
    "echo Start download\n",
    "\n",
    "wget --content-disposition \"https://data.census.gov/api/access/table/download?download_id=9a9fc9754aa170e9be08c6167bb61b1aa4233c29b3689db6a6f9b71ac8cb6be9\" -O datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip\n",
    "\n",
    "unzip -j \"datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip\" \"DECENNIALDHC2020.P1-Data.csv\" -d \"datasets/raw/census\"\n",
    "\n",
    "rm datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip"
   ],
   "id": "106fe85075db0389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-11-28 14:01:28--  https://data.census.gov/api/access/table/download?download_id=9a9fc9754aa170e9be08c6167bb61b1aa4233c29b3689db6a6f9b71ac8cb6be9\n",
      "Resolving data.census.gov (data.census.gov)... 2.19.140.252\n",
      "Connecting to data.census.gov (data.census.gov)|2.19.140.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 251463 (246K) [application/zip]\n",
      "Saving to: ‘datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 20% 2.16M 0s\n",
      "    50K .......... .......... .......... .......... .......... 40% 2.89M 0s\n",
      "   100K .......... .......... .......... .......... .......... 61% 3.14M 0s\n",
      "   150K .......... .......... .......... .......... .......... 81% 4.31M 0s\n",
      "   200K .......... .......... .......... .......... .....     100% 11.2M=0.07s\n",
      "\n",
      "2024-11-28 14:01:28 (3.41 MB/s) - ‘datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip’ saved [251463/251463]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/raw/census/DECENNIALDHC2020.P1-Data.csv.zip\n",
      "  inflating: datasets/raw/census/DECENNIALDHC2020.P1-Data.csv  \n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 8. Daily local weather data by city\n",
    "\n",
    "Manually download data to 2022 and 2023 for Seattle, Colorado, Denver, Albuquerque, Las Vegas and save as\n",
    "\n",
    "- datasets/raw/meteostat/albuquerque_2022.csv\n",
    "- datasets/raw/meteostat/albuquerque_2023.csv\n",
    "- datasets/raw/meteostat/columbus_2022.csv\n",
    "- datasets/raw/meteostat/columbus_2023.csv\n",
    "- datasets/raw/meteostat/denver_2022.csv\n",
    "- datasets/raw/meteostat/denver_2023.csv\n",
    "- datasets/raw/meteostat/las_vegas_2022.csv\n",
    "- datasets/raw/meteostat/las_vegas_2023.csv\n",
    "- datasets/raw/meteostat/seattle_2022.csv\n",
    "- datasets/raw/meteostat/seattle_2023.csv\n"
   ],
   "id": "b090c9e33b904563"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 9. Process Land Surface Temperature data\n",
    "\n",
    "Original data are stored as a raster geo data in HDFv4 format.\n",
    "The step extracts average LST for each ZIP code of the cities and saves the data as CSV file"
   ],
   "id": "cf87592d0b2dfe84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:24:56.539257Z",
     "start_time": "2024-11-28T10:56:20.744742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process LST data\n",
    "process_lst_data(LST_DATA_DIR, f\"{PROCESSED_LST_DATA_DIR}/cities_lst.csv\")\n",
    "\n"
   ],
   "id": "cf11fb048ad2c9e1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MODIS LST files: 100%|██████████| 2861/2861 [1:22:56<00:00,  1.74s/it]\n",
      "Processing seattle files: 100%|██████████| 712/712 [00:05<00:00, 135.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved as seattle.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing las_vegas files: 100%|██████████| 715/715 [00:16<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved as las_vegas.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing albuquerque files: 100%|██████████| 716/716 [00:06<00:00, 107.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved as albuquerque.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing denver files: 100%|██████████| 716/716 [00:08<00:00, 83.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved as denver.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing columbus files: 100%|██████████| 714/714 [00:10<00:00, 71.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GeoJSON saved as columbus.geojson.\n",
      "Processing file: albuquerque.geojson\n",
      "Processing file: seattle.geojson\n",
      "Processing file: las_vegas.geojson\n",
      "Processing file: columbus.geojson\n",
      "Processing file: denver.geojson\n",
      "Consolidated GeoJSON saved to: cities_lst.geojson\n",
      "Processed 2860 files.\n",
      "Loading GeoJSON file...\n",
      "Dropping geometry column...\n",
      "Saving to CSV...\n",
      "CSV file saved at 'cities_lst.csv'.\n",
      "Loading CSV file...\n",
      "Renaming column 'ZCTA5CE20' to 'zipcode'...\n",
      "Saving updated CSV...\n",
      "Updated CSV saved at 'cities_lst.csv'.\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 10. Process Normalized Difference Vegetation Index data\n",
    "Original data are stored as a raster geo data in HDFv4 format. The step extracts average NDVI for each ZIP code of the cities and saves the data as CSV file"
   ],
   "id": "c0dd474c02d92c93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:55.862493Z",
     "start_time": "2024-11-28T17:49:59.304443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process VI data\n",
    "process_vi_data(VI_DATA_DIR, f\"{PROCESSED_VI_DATA_DIR}/cities_vi.csv\")"
   ],
   "id": "fc56df19201810e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MODIS VI files: 100%|██████████| 189/189 [05:43<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 188 files.\n",
      "Processing file: seattle_NDVI_20230306.geojson\n",
      "Processing file: seattle_NDVI_20220117.geojson\n",
      "Processing file: albuquerque_NDVI_20220322.geojson\n",
      "Processing file: denver_NDVI_20220525.geojson\n",
      "Processing file: albuquerque_NDVI_20230813.geojson\n",
      "Processing file: denver_NDVI_20220218.geojson\n",
      "Processing file: columbus_NDVI_20230101.geojson\n",
      "Processing file: columbus_NDVI_20230914.geojson\n",
      "Processing file: seattle_NDVI_20221101.geojson\n",
      "Processing file: albuquerque_NDVI_20230218.geojson\n",
      "Processing file: denver_NDVI_20220813.geojson\n",
      "Processing file: albuquerque_NDVI_20230525.geojson\n",
      "Processing file: las_vegas_NDVI_20220610.geojson\n",
      "Processing file: denver_NDVI_20230322.geojson\n",
      "Processing file: columbus_NDVI_20231203.geojson\n",
      "Processing file: columbus_NDVI_20231117.geojson\n",
      "Processing file: denver_NDVI_20230930.geojson\n",
      "Processing file: columbus_NDVI_20230306.geojson\n",
      "Processing file: columbus_NDVI_20220117.geojson\n",
      "Processing file: las_vegas_NDVI_20220626.geojson\n",
      "Processing file: albuquerque_NDVI_20220712.geojson\n",
      "Processing file: seattle_NDVI_20230101.geojson\n",
      "Processing file: albuquerque_NDVI_20221016.geojson\n",
      "Processing file: columbus_NDVI_20221101.geojson\n",
      "Processing file: denver_NDVI_20230712.geojson\n",
      "Processing file: seattle_NDVI_20230914.geojson\n",
      "Processing file: las_vegas_NDVI_20221219.geojson\n",
      "Processing file: denver_NDVI_20231016.geojson\n",
      "Processing file: seattle_NDVI_20231203.geojson\n",
      "Processing file: seattle_NDVI_20231117.geojson\n",
      "Processing file: albuquerque_NDVI_20220930.geojson\n",
      "Processing file: las_vegas_NDVI_20220407.geojson\n",
      "Processing file: columbus_NDVI_20230829.geojson\n",
      "Processing file: albuquerque_NDVI_20220407.geojson\n",
      "Processing file: seattle_NDVI_20220728.geojson\n",
      "Processing file: las_vegas_NDVI_20220930.geojson\n",
      "Processing file: albuquerque_NDVI_20221219.geojson\n",
      "Processing file: seattle_NDVI_20230423.geojson\n",
      "Processing file: denver_NDVI_20230626.geojson\n",
      "Processing file: columbus_NDVI_20220202.geojson\n",
      "Processing file: seattle_NDVI_20220509.geojson\n",
      "Processing file: albuquerque_NDVI_20220626.geojson\n",
      "Processing file: las_vegas_NDVI_20221016.geojson\n",
      "Processing file: denver_NDVI_20231219.geojson\n",
      "Processing file: las_vegas_NDVI_20220712.geojson\n",
      "Processing file: denver_NDVI_20230407.geojson\n",
      "Processing file: seattle_NDVI_20230829.geojson\n",
      "Processing file: columbus_NDVI_20220728.geojson\n",
      "Processing file: columbus_NDVI_20230423.geojson\n",
      "Processing file: las_vegas_NDVI_20230525.geojson\n",
      "Processing file: las_vegas_NDVI_20230218.geojson\n",
      "Processing file: albuquerque_NDVI_20220610.geojson\n",
      "Processing file: seattle_NDVI_20220202.geojson\n",
      "Processing file: columbus_NDVI_20220509.geojson\n",
      "Processing file: denver_NDVI_20230610.geojson\n",
      "Processing file: las_vegas_NDVI_20220322.geojson\n",
      "Processing file: las_vegas_NDVI_20230813.geojson\n",
      "Processing file: denver_NDVI_20230829.geojson\n",
      "Processing file: las_vegas_NDVI_20220306.geojson\n",
      "Processing file: las_vegas_NDVI_20230117.geojson\n",
      "Processing file: columbus_NDVI_20230626.geojson\n",
      "Processing file: denver_NDVI_20220202.geojson\n",
      "Processing file: columbus_NDVI_20231219.geojson\n",
      "Processing file: las_vegas_NDVI_20231101.geojson\n",
      "Processing file: albuquerque_NDVI_20230202.geojson\n",
      "Processing file: seattle_NDVI_20230610.geojson\n",
      "Processing file: columbus_NDVI_20230407.geojson\n",
      "Processing file: albuquerque_NDVI_20220829.geojson\n",
      "Processing file: denver_NDVI_20220728.geojson\n",
      "Processing file: seattle_NDVI_20230626.geojson\n",
      "Processing file: albuquerque_NDVI_20230509.geojson\n",
      "Processing file: denver_NDVI_20230423.geojson\n",
      "Processing file: las_vegas_NDVI_20220101.geojson\n",
      "Processing file: albuquerque_NDVI_20220423.geojson\n",
      "Processing file: las_vegas_NDVI_20220914.geojson\n",
      "Processing file: seattle_NDVI_20231219.geojson\n",
      "Processing file: albuquerque_NDVI_20211219.geojson\n",
      "Processing file: las_vegas_NDVI_20221203.geojson\n",
      "Processing file: denver_NDVI_20220509.geojson\n",
      "Processing file: columbus_NDVI_20230610.geojson\n",
      "Processing file: las_vegas_NDVI_20221117.geojson\n",
      "Processing file: albuquerque_NDVI_20230728.geojson\n",
      "Processing file: seattle_NDVI_20230407.geojson\n",
      "Processing file: albuquerque_NDVI_20221117.geojson\n",
      "Processing file: las_vegas_NDVI_20230728.geojson\n",
      "Processing file: seattle_NDVI_20230930.geojson\n",
      "Processing file: las_vegas_NDVI_20211219.geojson\n",
      "Processing file: albuquerque_NDVI_20220914.geojson\n",
      "Processing file: las_vegas_NDVI_20220423.geojson\n",
      "Processing file: columbus_NDVI_20220525.geojson\n",
      "Processing file: columbus_NDVI_20220218.geojson\n",
      "Processing file: denver_NDVI_20230101.geojson\n",
      "Processing file: albuquerque_NDVI_20221203.geojson\n",
      "Processing file: las_vegas_NDVI_20230509.geojson\n",
      "Processing file: columbus_NDVI_20230322.geojson\n",
      "Processing file: denver_NDVI_20231203.geojson\n",
      "Processing file: denver_NDVI_20230914.geojson\n",
      "Processing file: seattle_NDVI_20231016.geojson\n",
      "Processing file: albuquerque_NDVI_20220101.geojson\n",
      "Processing file: seattle_NDVI_20230712.geojson\n",
      "Processing file: columbus_NDVI_20220813.geojson\n",
      "Processing file: denver_NDVI_20231117.geojson\n",
      "Processing file: denver_NDVI_20220117.geojson\n",
      "Processing file: denver_NDVI_20230306.geojson\n",
      "Processing file: columbus_NDVI_20230930.geojson\n",
      "Processing file: las_vegas_NDVI_20220829.geojson\n",
      "Processing file: seattle_NDVI_20220525.geojson\n",
      "Processing file: seattle_NDVI_20220218.geojson\n",
      "Processing file: albuquerque_NDVI_20231101.geojson\n",
      "Processing file: las_vegas_NDVI_20230202.geojson\n",
      "Processing file: seattle_NDVI_20230322.geojson\n",
      "Processing file: denver_NDVI_20221101.geojson\n",
      "Processing file: columbus_NDVI_20230712.geojson\n",
      "Processing file: seattle_NDVI_20220813.geojson\n",
      "Processing file: columbus_NDVI_20231016.geojson\n",
      "Processing file: albuquerque_NDVI_20230117.geojson\n",
      "Processing file: albuquerque_NDVI_20220306.geojson\n",
      "Processing file: las_vegas_NDVI_20220813.geojson\n",
      "Processing file: las_vegas_NDVI_20230322.geojson\n",
      "Processing file: columbus_NDVI_20230509.geojson\n",
      "Processing file: denver_NDVI_20220610.geojson\n",
      "Processing file: columbus_NDVI_20230728.geojson\n",
      "Processing file: seattle_NDVI_20220829.geojson\n",
      "Processing file: albuquerque_NDVI_20230610.geojson\n",
      "Processing file: seattle_NDVI_20230202.geojson\n",
      "Processing file: columbus_NDVI_20211219.geojson\n",
      "Processing file: las_vegas_NDVI_20220218.geojson\n",
      "Processing file: columbus_NDVI_20220423.geojson\n",
      "Processing file: las_vegas_NDVI_20220525.geojson\n",
      "Processing file: las_vegas_NDVI_20230712.geojson\n",
      "Processing file: denver_NDVI_20221219.geojson\n",
      "Processing file: las_vegas_NDVI_20231016.geojson\n",
      "Processing file: albuquerque_NDVI_20230626.geojson\n",
      "Processing file: seattle_NDVI_20230509.geojson\n",
      "Processing file: denver_NDVI_20220407.geojson\n",
      "Processing file: las_vegas_NDVI_20230930.geojson\n",
      "Processing file: seattle_NDVI_20230728.geojson\n",
      "Processing file: columbus_NDVI_20220829.geojson\n",
      "Processing file: albuquerque_NDVI_20230407.geojson\n",
      "Processing file: columbus_NDVI_20230202.geojson\n",
      "Processing file: denver_NDVI_20220626.geojson\n",
      "Processing file: seattle_NDVI_20220423.geojson\n",
      "Processing file: albuquerque_NDVI_20231219.geojson\n",
      "Processing file: seattle_NDVI_20211219.geojson\n",
      "Processing file: seattle_NDVI_20221203.geojson\n",
      "Processing file: denver_NDVI_20221016.geojson\n",
      "Processing file: las_vegas_NDVI_20231219.geojson\n",
      "Processing file: seattle_NDVI_20220914.geojson\n",
      "Processing file: columbus_NDVI_20231101.geojson\n",
      "Processing file: denver_NDVI_20220712.geojson\n",
      "Processing file: las_vegas_NDVI_20230407.geojson\n",
      "Processing file: albuquerque_NDVI_20230930.geojson\n",
      "Processing file: seattle_NDVI_20221117.geojson\n",
      "Processing file: columbus_NDVI_20230117.geojson\n",
      "Processing file: columbus_NDVI_20220306.geojson\n",
      "Processing file: denver_NDVI_20220930.geojson\n",
      "Processing file: albuquerque_NDVI_20231016.geojson\n",
      "Processing file: albuquerque_NDVI_20230712.geojson\n",
      "Processing file: seattle_NDVI_20220101.geojson\n",
      "Processing file: las_vegas_NDVI_20230626.geojson\n",
      "Processing file: columbus_NDVI_20221203.geojson\n",
      "Processing file: denver_NDVI_20220322.geojson\n",
      "Processing file: las_vegas_NDVI_20230610.geojson\n",
      "Processing file: albuquerque_NDVI_20220525.geojson\n",
      "Processing file: denver_NDVI_20230813.geojson\n",
      "Processing file: seattle_NDVI_20231101.geojson\n",
      "Processing file: albuquerque_NDVI_20220218.geojson\n",
      "Processing file: columbus_NDVI_20220914.geojson\n",
      "Processing file: columbus_NDVI_20221117.geojson\n",
      "Processing file: seattle_NDVI_20230117.geojson\n",
      "Processing file: seattle_NDVI_20220306.geojson\n",
      "Processing file: denver_NDVI_20230218.geojson\n",
      "Processing file: columbus_NDVI_20220101.geojson\n",
      "Processing file: albuquerque_NDVI_20220813.geojson\n",
      "Processing file: denver_NDVI_20230525.geojson\n",
      "Processing file: albuquerque_NDVI_20230322.geojson\n",
      "Processing file: columbus_NDVI_20221016.geojson\n",
      "Processing file: seattle_NDVI_20230813.geojson\n",
      "Processing file: denver_NDVI_20231101.geojson\n",
      "Processing file: columbus_NDVI_20220712.geojson\n",
      "Processing file: seattle_NDVI_20220322.geojson\n",
      "Processing file: albuquerque_NDVI_20230306.geojson\n",
      "Processing file: albuquerque_NDVI_20220117.geojson\n",
      "Processing file: las_vegas_NDVI_20230829.geojson\n",
      "Processing file: columbus_NDVI_20220930.geojson\n",
      "Processing file: denver_NDVI_20220306.geojson\n",
      "Processing file: denver_NDVI_20230117.geojson\n",
      "Processing file: las_vegas_NDVI_20220202.geojson\n",
      "Processing file: seattle_NDVI_20230218.geojson\n",
      "Processing file: albuquerque_NDVI_20221101.geojson\n",
      "Processing file: seattle_NDVI_20230525.geojson\n",
      "Processing file: columbus_NDVI_20230813.geojson\n",
      "Processing file: albuquerque_NDVI_20230101.geojson\n",
      "Processing file: seattle_NDVI_20220712.geojson\n",
      "Processing file: seattle_NDVI_20221016.geojson\n",
      "Processing file: denver_NDVI_20220914.geojson\n",
      "Processing file: denver_NDVI_20221203.geojson\n",
      "Processing file: columbus_NDVI_20220322.geojson\n",
      "Processing file: las_vegas_NDVI_20220509.geojson\n",
      "Processing file: denver_NDVI_20221117.geojson\n",
      "Processing file: seattle_NDVI_20220930.geojson\n",
      "Processing file: las_vegas_NDVI_20220728.geojson\n",
      "Processing file: albuquerque_NDVI_20231117.geojson\n",
      "Processing file: albuquerque_NDVI_20231203.geojson\n",
      "Processing file: columbus_NDVI_20230218.geojson\n",
      "Processing file: denver_NDVI_20220101.geojson\n",
      "Processing file: las_vegas_NDVI_20230423.geojson\n",
      "Processing file: columbus_NDVI_20230525.geojson\n",
      "Processing file: albuquerque_NDVI_20230914.geojson\n",
      "Processing file: denver_NDVI_20230509.geojson\n",
      "Processing file: columbus_NDVI_20220610.geojson\n",
      "Processing file: las_vegas_NDVI_20231203.geojson\n",
      "Processing file: seattle_NDVI_20221219.geojson\n",
      "Processing file: las_vegas_NDVI_20230914.geojson\n",
      "Processing file: albuquerque_NDVI_20230423.geojson\n",
      "Processing file: seattle_NDVI_20220407.geojson\n",
      "Processing file: albuquerque_NDVI_20220728.geojson\n",
      "Processing file: las_vegas_NDVI_20231117.geojson\n",
      "Processing file: denver_NDVI_20230728.geojson\n",
      "Processing file: denver_NDVI_20211219.geojson\n",
      "Processing file: las_vegas_NDVI_20230101.geojson\n",
      "Processing file: denver_NDVI_20220423.geojson\n",
      "Processing file: albuquerque_NDVI_20220509.geojson\n",
      "Processing file: seattle_NDVI_20220626.geojson\n",
      "Processing file: albuquerque_NDVI_20220202.geojson\n",
      "Processing file: seattle_NDVI_20220610.geojson\n",
      "Processing file: las_vegas_NDVI_20221101.geojson\n",
      "Processing file: columbus_NDVI_20221219.geojson\n",
      "Processing file: columbus_NDVI_20220407.geojson\n",
      "Processing file: albuquerque_NDVI_20230829.geojson\n",
      "Processing file: las_vegas_NDVI_20220117.geojson\n",
      "Processing file: las_vegas_NDVI_20230306.geojson\n",
      "Processing file: denver_NDVI_20220829.geojson\n",
      "Processing file: denver_NDVI_20230202.geojson\n",
      "Processing file: columbus_NDVI_20220626.geojson\n",
      "Consolidated GeoJSON saved to: cities_vi.geojson\n",
      "Loading GeoJSON file...\n",
      "Dropping geometry column...\n",
      "Saving to CSV...\n",
      "CSV file saved at 'cities_vi.csv'.\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 11. Process satellite meteo data\n",
    "The data are in netCDF format. The step extracts the data and saves as CSV file"
   ],
   "id": "8889367f727eb310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:12:09.824778Z",
     "start_time": "2024-11-28T18:00:07.328098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process datamet data\n",
    "process_daymet_data(DAYMET_DATA_DIR)"
   ],
   "id": "46bfc9615d6d2d07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZIP code shapefile...\n",
      "Processing city: ALBUQUERQUE\n",
      "  Processing tile: 11197\n",
      "    Processing file: 11197_2023_dayl.nc\n",
      "    Processing file: 11197_2022_dayl.nc\n",
      "    Processing file: 11197_2023_prcp.nc\n",
      "    Processing file: 11197_2022_prcp.nc\n",
      "    Processing file: 11197_2023_srad.nc\n",
      "    Processing file: 11197_2022_srad.nc\n",
      "    Processing file: 11197_2023_swe.nc\n",
      "    Processing file: 11197_2022_swe.nc\n",
      "    Processing file: 11197_2022_tmax.nc\n",
      "    Processing file: 11197_2023_tmax.nc\n",
      "    Processing file: 11197_2022_tmin.nc\n",
      "    Processing file: 11197_2023_tmin.nc\n",
      "    Processing file: 11197_2022_vp.nc\n",
      "    Processing file: 11197_2023_vp.nc\n",
      "  Aggregating data for variable 'dayl'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_dayl.csv'\n",
      "  Aggregating data for variable 'prcp'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_prcp.csv'\n",
      "  Aggregating data for variable 'srad'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_srad.csv'\n",
      "  Aggregating data for variable 'swe'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_swe.csv'\n",
      "  Aggregating data for variable 'tmax'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_tmax.csv'\n",
      "  Aggregating data for variable 'tmin'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_tmin.csv'\n",
      "  Aggregating data for variable 'vp'\n",
      "    Saved aggregated data to 'ALBUQUERQUE_zipcode_avg_vp.csv'\n",
      "Finished processing city: ALBUQUERQUE\n",
      "Processing city: COLUMBUS\n",
      "  Processing tile: 11569\n",
      "    Processing file: 11569_2022_dayl.nc\n",
      "    Processing file: 11569_2023_dayl.nc\n",
      "    Processing file: 11569_2022_prcp.nc\n",
      "    Processing file: 11569_2023_prcp.nc\n",
      "    Processing file: 11569_2022_srad.nc\n",
      "    Processing file: 11569_2023_srad.nc\n",
      "    Processing file: 11569_2023_swe.nc\n",
      "    Processing file: 11569_2022_swe.nc\n",
      "    Processing file: 11569_2023_tmax.nc\n",
      "    Processing file: 11569_2022_tmax.nc\n",
      "    Processing file: 11569_2023_tmin.nc\n",
      "    Processing file: 11569_2022_tmin.nc\n",
      "    Processing file: 11569_2023_vp.nc\n",
      "    Processing file: 11569_2022_vp.nc\n",
      "  Processing tile: 11749\n",
      "    Processing file: 11749_2023_dayl.nc\n",
      "    Processing file: 11749_2022_dayl.nc\n",
      "    Processing file: 11749_2023_prcp.nc\n",
      "    Processing file: 11749_2022_prcp.nc\n",
      "    Processing file: 11749_2023_srad.nc\n",
      "    Processing file: 11749_2022_srad.nc\n",
      "    Processing file: 11749_2023_swe.nc\n",
      "    Processing file: 11749_2022_swe.nc\n",
      "    Processing file: 11749_2022_tmax.nc\n",
      "    Processing file: 11749_2023_tmax.nc\n",
      "    Processing file: 11749_2022_tmin.nc\n",
      "    Processing file: 11749_2023_tmin.nc\n",
      "    Processing file: 11749_2023_vp.nc\n",
      "    Processing file: 11749_2022_vp.nc\n",
      "  Aggregating data for variable 'dayl'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_dayl.csv'\n",
      "  Aggregating data for variable 'prcp'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_prcp.csv'\n",
      "  Aggregating data for variable 'srad'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_srad.csv'\n",
      "  Aggregating data for variable 'swe'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_swe.csv'\n",
      "  Aggregating data for variable 'tmax'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_tmax.csv'\n",
      "  Aggregating data for variable 'tmin'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_tmin.csv'\n",
      "  Aggregating data for variable 'vp'\n",
      "    Saved aggregated data to 'COLUMBUS_zipcode_avg_vp.csv'\n",
      "Finished processing city: COLUMBUS\n",
      "Processing city: DENVER\n",
      "  Processing tile: 11558\n",
      "    Processing file: 11558_2022_dayl.nc\n",
      "    Processing file: 11558_2023_dayl.nc\n",
      "    Processing file: 11558_2022_prcp.nc\n",
      "    Processing file: 11558_2023_prcp.nc\n",
      "    Processing file: 11558_2022_srad.nc\n",
      "    Processing file: 11558_2023_srad.nc\n",
      "    Processing file: 11558_2023_swe.nc\n",
      "    Processing file: 11558_2022_swe.nc\n",
      "    Processing file: 11558_2023_tmax.nc\n",
      "    Processing file: 11558_2022_tmax.nc\n",
      "    Processing file: 11558_2023_tmin.nc\n",
      "    Processing file: 11558_2022_tmin.nc\n",
      "    Processing file: 11558_2022_vp.nc\n",
      "    Processing file: 11558_2023_vp.nc\n",
      "  Aggregating data for variable 'dayl'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_dayl.csv'\n",
      "  Aggregating data for variable 'prcp'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_prcp.csv'\n",
      "  Aggregating data for variable 'srad'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_srad.csv'\n",
      "  Aggregating data for variable 'swe'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_swe.csv'\n",
      "  Aggregating data for variable 'tmax'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_tmax.csv'\n",
      "  Aggregating data for variable 'tmin'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_tmin.csv'\n",
      "  Aggregating data for variable 'vp'\n",
      "    Saved aggregated data to 'DENVER_zipcode_avg_vp.csv'\n",
      "Finished processing city: DENVER\n",
      "Processing city: LAS VEGAS\n",
      "  Processing tile: 11193\n",
      "    Processing file: 11193_2023_dayl.nc\n",
      "    Processing file: 11193_2022_dayl.nc\n",
      "    Processing file: 11193_2023_prcp.nc\n",
      "    Processing file: 11193_2022_prcp.nc\n",
      "    Processing file: 11193_2023_srad.nc\n",
      "    Processing file: 11193_2022_srad.nc\n",
      "    Processing file: 11193_2022_swe.nc\n",
      "    Processing file: 11193_2023_swe.nc\n",
      "    Processing file: 11193_2022_tmax.nc\n",
      "    Processing file: 11193_2023_tmax.nc\n",
      "    Processing file: 11193_2022_tmin.nc\n",
      "    Processing file: 11193_2023_tmin.nc\n",
      "    Processing file: 11193_2022_vp.nc\n",
      "    Processing file: 11193_2023_vp.nc\n",
      "  Processing tile: 11373\n",
      "    Processing file: 11373_2023_dayl.nc\n",
      "    Processing file: 11373_2022_dayl.nc\n",
      "    Processing file: 11373_2023_prcp.nc\n",
      "    Processing file: 11373_2022_prcp.nc\n",
      "    Processing file: 11373_2023_srad.nc\n",
      "    Processing file: 11373_2022_srad.nc\n",
      "    Processing file: 11373_2022_swe.nc\n",
      "    Processing file: 11373_2023_swe.nc\n",
      "    Processing file: 11373_2022_tmax.nc\n",
      "    Processing file: 11373_2023_tmax.nc\n",
      "    Processing file: 11373_2022_tmin.nc\n",
      "    Processing file: 11373_2023_tmin.nc\n",
      "    Processing file: 11373_2022_vp.nc\n",
      "    Processing file: 11373_2023_vp.nc\n",
      "  Aggregating data for variable 'dayl'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_dayl.csv'\n",
      "  Aggregating data for variable 'prcp'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_prcp.csv'\n",
      "  Aggregating data for variable 'srad'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_srad.csv'\n",
      "  Aggregating data for variable 'swe'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_swe.csv'\n",
      "  Aggregating data for variable 'tmax'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_tmax.csv'\n",
      "  Aggregating data for variable 'tmin'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_tmin.csv'\n",
      "  Aggregating data for variable 'vp'\n",
      "    Saved aggregated data to 'LAS VEGAS_zipcode_avg_vp.csv'\n",
      "Finished processing city: LAS VEGAS\n",
      "Processing city: SEATTLE\n",
      "  Processing tile: 12269\n",
      "    Processing file: 12269_2023_dayl.nc\n",
      "    Processing file: 12269_2022_dayl.nc\n",
      "    Processing file: 12269_2023_prcp.nc\n",
      "    Processing file: 12269_2022_prcp.nc\n",
      "    Processing file: 12269_2023_srad.nc\n",
      "    Processing file: 12269_2022_srad.nc\n",
      "    Processing file: 12269_2023_swe.nc\n",
      "    Processing file: 12269_2022_swe.nc\n",
      "    Processing file: 12269_2022_tmax.nc\n",
      "    Processing file: 12269_2023_tmax.nc\n",
      "    Processing file: 12269_2022_tmin.nc\n",
      "    Processing file: 12269_2023_tmin.nc\n",
      "    Processing file: 12269_2023_vp.nc\n",
      "    Processing file: 12269_2022_vp.nc\n",
      "  Aggregating data for variable 'dayl'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_dayl.csv'\n",
      "  Aggregating data for variable 'prcp'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_prcp.csv'\n",
      "  Aggregating data for variable 'srad'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_srad.csv'\n",
      "  Aggregating data for variable 'swe'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_swe.csv'\n",
      "  Aggregating data for variable 'tmax'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_tmax.csv'\n",
      "  Aggregating data for variable 'tmin'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_tmin.csv'\n",
      "  Aggregating data for variable 'vp'\n",
      "    Saved aggregated data to 'SEATTLE_zipcode_avg_vp.csv'\n",
      "Finished processing city: SEATTLE\n",
      "All cities processed.\n",
      "Processing files...\n",
      "Combining data...\n",
      "Saving combined data to CSV...\n",
      "Combined data saved to 'combined_meteo_sat_data.csv'.\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 12. Process population density data",
   "id": "b4c4832f6d2363cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:41:20.895108Z",
     "start_time": "2024-11-28T20:41:18.187627Z"
    }
   },
   "cell_type": "code",
   "source": "process_population_data()",
   "id": "ffe68b1115ddf36c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading population data...\n",
      "Filtered population data rows: 150\n",
      "Loading ZCTA shapefile...\n",
      "Filtered ZCTA rows: 150\n",
      "Common ZIP codes: 150\n",
      "Merging population data with ZCTA geometry...\n",
      "Merged rows: 150\n",
      "Filtered population density data saved to 'zipcode_population_density_filtered.csv'.\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 13. Process local meteostat data",
   "id": "5e2efefe95702021"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:41:53.728280Z",
     "start_time": "2024-11-28T20:41:53.685329Z"
    }
   },
   "cell_type": "code",
   "source": "process_meteostat_data()",
   "id": "392b03774e5a8388",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSV files...\n",
      "Saving combined data to CSV...\n",
      "Combined data saved to 'combined_meteo_data_with_city.csv'.\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 14. process Land Cover data\n",
    "The data are in GeoTIFF format. The step extracts the data and saves as CSV file"
   ],
   "id": "2e864c6109fcf424"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:45:11.623073Z",
     "start_time": "2024-11-28T20:45:03.511233Z"
    }
   },
   "cell_type": "code",
   "source": "process_lc_data()",
   "id": "bd08288a3fea856b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZIP code shapefile...\n",
      "Original ZCTA CRS: EPSG:4269\n",
      "Raster CRS: PROJCS[\"AEA        WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "CRS mismatch detected. Reprojecting shapefile to raster CRS...\n",
      "Reprojection complete.\n",
      "Filtering ZCTA GeoDataFrame for target ZIP codes...\n",
      "Filtered ZIP codes count: 150\n",
      "Calculating land cover statistics...\n",
      "Results saved to 'zipcode_land_cover_percentages.csv'.\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 15. Process Normalized Difference Vegetation Index data\n",
    "The data are in GeoTIFF format. The step extracts the data and saves as CSV file"
   ],
   "id": "ed24fbc476bfa7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:45:50.266534Z",
     "start_time": "2024-11-28T20:45:42.303252Z"
    }
   },
   "cell_type": "code",
   "source": "process_fis_data()",
   "id": "b232014ea91612e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZIP code shapefile...\n",
      "Filtered to 150 ZIP codes for the specified cities.\n",
      "Calculating ISA metrics for each ZIP code...\n",
      "Corrected ISA metrics saved to 'zipcode_impervious_surface_area_corrected.csv'\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 16. Assemble final dataset\n",
    "The step combines pre-processed source data into a single dataset and saves as CSV file data/processed/uhi.csv "
   ],
   "id": "8d53959fd1541ab3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:53:55.410461Z",
     "start_time": "2024-11-28T23:53:54.940156Z"
    }
   },
   "cell_type": "code",
   "source": "create_final_dataset()",
   "id": "afaaf8af993fd1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Selecting required columns from meteo_per_city...\n",
      "Merging lst_df with meteo_sat_df ...\n",
      "Dataset saved to 'uhi.csv'\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0eca690e247ba6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
